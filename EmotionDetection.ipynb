{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5814268a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "No GPU detected — running on CPU (training will be slower).\n",
      "Environment initialized successfully.\n",
      "Mixed precision: <DTypePolicy \"float32\">\n"
     ]
    }
   ],
   "source": [
    "#STEP 1: Environment Setup + GPU Verification\n",
    "import os, time\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# --- Check for GPU availability ---\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPU detected: {gpus[0].name}\")\n",
    "    try:\n",
    "        tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "        print(\"Mixed precision enabled for performance boost.\")\n",
    "    except Exception as e:\n",
    "        print(\"Mixed precision could not be set:\", e)\n",
    "else:\n",
    "    print(\"No GPU detected — running on CPU (training will be slower).\")\n",
    "\n",
    "tf.config.threading.set_inter_op_parallelism_threads(4)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(4)\n",
    "\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(\"Environment initialized successfully.\")\n",
    "print(\"Mixed precision:\", tf.keras.mixed_precision.global_policy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd5af978",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Train directory not found: Dataset/train",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(ALT_TRAIN):\n\u001b[32m     11\u001b[39m         TRAIN_DIR, TEST_DIR = ALT_TRAIN, ALT_TEST\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m os.path.exists(TRAIN_DIR), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain directory not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTRAIN_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m os.path.exists(TEST_DIR),  \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest directory not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTEST_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDataset directories verified successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAssertionError\u001b[39m: Train directory not found: Dataset/train"
     ]
    }
   ],
   "source": [
    "# STEP 2: Dataset Verification & Path Setup \n",
    "import os\n",
    "\n",
    "TRAIN_DIR = 'Dataset/train'\n",
    "TEST_DIR  = 'Dataset/test'\n",
    "\n",
    "if not os.path.exists(TRAIN_DIR):\n",
    "    ALT_TRAIN = 'input/train'\n",
    "    ALT_TEST  = 'input/test'\n",
    "    if os.path.exists(ALT_TRAIN):\n",
    "        TRAIN_DIR, TEST_DIR = ALT_TRAIN, ALT_TEST\n",
    "\n",
    "assert os.path.exists(TRAIN_DIR), f\"Train directory not found: {TRAIN_DIR}\"\n",
    "assert os.path.exists(TEST_DIR),  f\"Test directory not found: {TEST_DIR}\"\n",
    "\n",
    "print(\"Dataset directories verified successfully.\")\n",
    "print(f\"Train path : {TRAIN_DIR}\")\n",
    "print(f\"Test path  : {TEST_DIR}\")\n",
    "print(\"Train classes:\", sorted(os.listdir(TRAIN_DIR)))\n",
    "print(\"Test classes :\", sorted(os.listdir(TEST_DIR)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce9d7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Data Loading + Preprocessing \n",
    "import os, math, tensorflow as tf\n",
    "\n",
    "# --- Config ---\n",
    "IMG_SIZE = (48, 48)\n",
    "BATCH_SIZE = 64        \n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# --- Dataset paths (auto-check for both working/input dirs) ---\n",
    "train_dir = 'Dataset/train'\n",
    "test_dir  = 'Dataset/test'\n",
    "\n",
    "if not os.path.exists(train_dir):\n",
    "    alt_train = 'input/train'\n",
    "    alt_test  = 'input/test'\n",
    "    if os.path.exists(alt_train):\n",
    "        train_dir, test_dir = alt_train, alt_test\n",
    "\n",
    "assert os.path.exists(train_dir), f\"Train path not found: {train_dir}\"\n",
    "assert os.path.exists(test_dir),  f\"Test path not found: {test_dir}\"\n",
    "\n",
    "print(f\" Dataset verified:\\n   Train: {train_dir}\\n   Test : {test_dir}\")\n",
    "\n",
    "# --- Create image datasets ---\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    color_mode='grayscale',\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    color_mode='grayscale',\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "# --- Data Augmentation (helps prevent overfitting, boosts generalization) ---\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "    tf.keras.layers.RandomTranslation(0.1, 0.1)\n",
    "])\n",
    "\n",
    "# --- Normalization Layer ---\n",
    "normalization = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "def preprocess_train(images, labels):\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    images = normalization(images)\n",
    "    images = data_augmentation(images)\n",
    "    return images, labels\n",
    "\n",
    "def preprocess_val(images, labels):\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    images = normalization(images)\n",
    "    return images, labels\n",
    "\n",
    "train_ds = (train_ds\n",
    "            .map(preprocess_train, num_parallel_calls=AUTOTUNE)\n",
    "            .cache()\n",
    "            .shuffle(2048)\n",
    "            .prefetch(AUTOTUNE))\n",
    "\n",
    "val_ds = (val_ds\n",
    "          .map(preprocess_val, num_parallel_calls=AUTOTUNE)\n",
    "          .cache()\n",
    "          .prefetch(AUTOTUNE))\n",
    "\n",
    "print(f\"tf.data pipeline ready | IMG_SIZE: {IMG_SIZE} | BATCH: {BATCH_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fe5da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Data Augmentation\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\", seed=42),\n",
    "    tf.keras.layers.RandomRotation(0.07, seed=42),     # slightly reduced\n",
    "    tf.keras.layers.RandomZoom(0.07, seed=42),\n",
    "    tf.keras.layers.RandomTranslation(0.05, 0.05, seed=42),\n",
    "], name=\"data_augmentation\")\n",
    "\n",
    "print(\"Data augmentation pipeline initialized successfully.\")\n",
    "\n",
    "# --- Optional visualization ---\n",
    "VISUALIZE = True  # set False during training to save time\n",
    "if VISUALIZE:\n",
    "    import numpy as np\n",
    "    for images, labels in train_ds.take(1):\n",
    "        aug_images = data_augmentation(images)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for i in range(12):\n",
    "            ax = plt.subplot(3, 4, i + 1)\n",
    "            plt.imshow(tf.squeeze(aug_images[i]), cmap='gray')\n",
    "            plt.axis(\"off\")\n",
    "        plt.suptitle(\"Augmented Samples\", fontsize=14)\n",
    "        plt.show()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c6f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Enhanced Lightweight CNN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, mixed_precision\n",
    "\n",
    "# --- GPU & precision ---\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(\"Mixed precision policy:\", mixed_precision.global_policy())\n",
    "\n",
    "# --- Build model ---\n",
    "def build_light_cnn(input_shape=(48,48,1), n_classes=7):\n",
    "    inputs = layers.Input(shape=input_shape, name=\"input_image\")\n",
    "    x = data_augmentation(inputs)\n",
    "\n",
    "    # Block 1\n",
    "    x = layers.Conv2D(32, 3, padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.1)(x)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = layers.Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.1)(x)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = layers.Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.1)(x)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    x = layers.Dropout(0.35)(x)\n",
    "\n",
    "    # Block 4 (extra compact feature extractor)\n",
    "    x = layers.Conv2D(256, 3, padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.1)(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Dense head\n",
    "    x = layers.Dense(128, kernel_initializer='he_normal')(x)\n",
    "    x = layers.LeakyReLU(0.1)(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "\n",
    "    outputs = layers.Dense(n_classes, activation='softmax', dtype='float32')(x)\n",
    "    return models.Model(inputs, outputs, name=\"LightCNN_Enhanced\")\n",
    "\n",
    "# --- Compile ---\n",
    "model = build_light_cnn()\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "print(f\"✅ Model parameters: {model.count_params():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4bea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: Callbacks \n",
    "import os, datetime, tensorflow as tf\n",
    "\n",
    "# --- Paths ---\n",
    "CHECKPOINT_DIR = \"working/checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"best_model.keras\")\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "LOG_DIR = os.path.join(CHECKPOINT_DIR, f\"logs_{timestamp}\")\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# --- Define Callbacks ---\n",
    "callbacks = [\n",
    "    # Best model saving\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=BEST_MODEL_PATH,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # Early stopping to prevent overfitting\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=6,\n",
    "        mode='max',\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # CSV logger for later visualization\n",
    "    tf.keras.callbacks.CSVLogger(\n",
    "        os.path.join(CHECKPOINT_DIR, \"training_log.csv\"),\n",
    "        append=False\n",
    "    ),\n",
    "\n",
    "    # TensorBoard (optional, skip if not needed)\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=LOG_DIR,\n",
    "        histogram_freq=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Callbacks configured successfully.\")\n",
    "print(f\"Best model path : {BEST_MODEL_PATH}\")\n",
    "print(f\"TensorBoard log : {LOG_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c663861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: Model Training — GPU Optimized & Resume-Safe\n",
    "import time, os, tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# ---- CONFIG ----\n",
    "EPOCHS = 25                    \n",
    "BEST_MODEL_PATH = \"working/checkpoints/best_model.keras\"\n",
    "\n",
    "# ---- Resume if checkpoint exists ----\n",
    "initial_epoch = 0\n",
    "if os.path.exists(BEST_MODEL_PATH):\n",
    "    try:\n",
    "        print(\"Found previous best model — loading for resume.\")\n",
    "        model = load_model(BEST_MODEL_PATH)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load previous checkpoint — starting fresh:\", e)\n",
    "\n",
    "# ---- Optional: Class Weights (to fix imbalance) ----\n",
    "# If you calculated class counts earlier, plug them here.\n",
    "class_weights = {\n",
    "    0: 1.5,  # angry\n",
    "    1: 1.0,  # disgust\n",
    "    2: 1.2,  # fear\n",
    "    3: 0.9,  # happy\n",
    "    4: 1.3,  # sad\n",
    "    5: 1.0,  # surprise\n",
    "    6: 1.1   # neutral\n",
    "}\n",
    "\n",
    "# ---- Timer Callback ----\n",
    "class EpochTimer(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start = time.time()\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"⏱ Epoch {epoch+1} — \"\n",
    "              f\"val_acc: {logs.get('val_accuracy'):.4f}, \"\n",
    "              f\"val_loss: {logs.get('val_loss'):.4f}\")\n",
    "    def on_train_end(self, logs=None):\n",
    "        print(f\"Total training time: {(time.time()-self.start)/60:.2f} min\")\n",
    "\n",
    "epoch_timer = EpochTimer()\n",
    "\n",
    "# ---- Train ----\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[*callbacks, epoch_timer],\n",
    "    class_weight=class_weights,     \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65c324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 8: Resume / Fine-Tune Training — GPU Optimized (Target 70–80%)\n",
    "import os, time, tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# === Paths ===\n",
    "CHECKPOINT_DIR = \"working/checkpoints\"\n",
    "BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"best_model.keras\")\n",
    "\n",
    "# === Fine-Tuning Config ===\n",
    "INITIAL_EPOCH = 3\n",
    "TOTAL_EPOCHS  = 25        # extended fine-tuning\n",
    "BASE_LR       = 1e-4\n",
    "FINE_TUNE_LR  = 1e-5      # smaller LR for unfreezing stage\n",
    "\n",
    "# === Load Best Model or Build Pretrained Backbone ===\n",
    "if os.path.exists(BEST_MODEL_PATH):\n",
    "    print(\"Found previous checkpoint — loading MobileNetV2 fine-tuning model.\")\n",
    "    model = load_model(BEST_MODEL_PATH)\n",
    "else:\n",
    "    print(\"No checkpoint found — building new fine-tuning model.\")\n",
    "\n",
    "    base_model = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=(48, 48, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg'\n",
    "    )\n",
    "    base_model.trainable = False  # freeze base initially\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(48, 48, 1))\n",
    "    x = tf.keras.layers.Conv2D(3, (3, 3), padding='same')(inputs)  # convert grayscale to RGB\n",
    "    x = data_augmentation(x)\n",
    "    x = tf.keras.applications.mobilenet_v2.preprocess_input(x)\n",
    "    x = base_model(x, training=False)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "    outputs = tf.keras.layers.Dense(7, activation='softmax', dtype='float32')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=BASE_LR),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "# === Stage 1: Warmup Training (frozen base) ===\n",
    "print(\"Stage 1: Warming up new classifier layers...\")\n",
    "warmup_history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    initial_epoch=INITIAL_EPOCH,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# === Stage 2: Fine-tuning top 50% of backbone ===\n",
    "fine_tune_at = len(model.layers) // 2\n",
    "for layer in model.layers[fine_tune_at:]:\n",
    "    if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        layer.trainable = True\n",
    "\n",
    "print(f\"Unfroze {len(model.layers) - fine_tune_at} layers for fine-tuning.\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=FINE_TUNE_LR),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# === Fine-tuning ===\n",
    "print(\" Stage 2: Fine-tuning with lower LR...\")\n",
    "start_time = time.time()\n",
    "\n",
    "history_ft = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    initial_epoch=10,\n",
    "    epochs=TOTAL_EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "elapsed = (time.time() - start_time) / 60\n",
    "print(f\"Fine-tuning completed in {elapsed:.2f} minutes.\")\n",
    "print(\"Expected Accuracy: 70–80% range with full GPU training.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
